{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffa757e87f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, IndexToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import lit # for the dummy _transform\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class UDShortWordsRemover(\n",
    "    Transformer, DefaultParamsReadable, DefaultParamsWritable,\n",
    "):\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(UDShortWordsRemover, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self):\n",
    "        \"\"\"\n",
    "        setParams(self, outputCols=None, value=0.0)\n",
    "        Sets params for this RegexReplacerWritable.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        df = df.withColumn(\"filtered2\", f.expr(\"filter(filtered, x -> not(length(x) < 3))\")).where(f.size(f.col(\"filtered2\")) > 0).drop(\"filtered\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import lit # for the dummy _transform\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "class UDLemmatization(\n",
    "    Transformer, DefaultParamsReadable, DefaultParamsWritable,\n",
    "):\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(UDLemmatization, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self):\n",
    "        \"\"\"\n",
    "        setParams(self, outputCols=None, value=0.0)\n",
    "        Sets params for this RegexReplacerWritable.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        pandas_df = df.select(\"*\").toPandas()\n",
    "        pandas_df['lemmatized'] = pandas_df['words'].apply(\n",
    "                    lambda lst:[lemmatizer.lemmatize(word) for word in lst])\n",
    "        pandas_df['lemmatized']\n",
    "        df = spark.createDataFrame(pandas_df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = True\n",
    "globals()['my_model'] = PipelineModel.load(\"lr_model\")\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    # Utilize our predict function\n",
    "   #df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
    "       #struct([df[x] for x in df.columns])\n",
    "    #))\n",
    "    #df_withpreds.show()\n",
    "        \n",
    "    # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict as we did here (you can)\n",
    "    # but an MLlib model you've built and saved with Spark\n",
    "    # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = '***' # Replace '***' with:    [...].load('my_logistic_regression')\n",
    "        globals()['models_loaded'] = True\n",
    "    \n",
    "    prediction = globals()['my_model'].transform(df)\n",
    "    \n",
    "    ind_str = IndexToString(inputCol='prediction',outputCol='labelPredicted',labels=globals()['my_model'].stages[7].labels)\n",
    "    prediction_text = ind_str.transform(prediction)\n",
    "    selected = prediction_text.select(\"labelOriginal\", \"labelPredicted\")\n",
    "    for row in selected.collect():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2021-05-28 20:13:20 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#covid|1398339107722387458|@RightWalkIndia @...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#covid', labelPredicted='#covid')\n",
      "========= 2021-05-28 20:13:40 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#covid|1398339107449970688|@rheytah This pic...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#covid', labelPredicted='#covid')\n",
      "========= 2021-05-28 20:13:50 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#covid|1398339099447209985|Todays's @CSOIrel...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#covid', labelPredicted='#covid')\n",
      "========= 2021-05-28 20:14:10 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#covid|1398339086063181827|@BasementJayBee @...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#covid', labelPredicted='#covid')\n",
      "========= 2021-05-28 20:14:20 =========\n",
      "+--------+-------------------+--------------------+\n",
      "|   label|           tweet_id|          tweet_text|\n",
      "+--------+-------------------+--------------------+\n",
      "|#vaccine|1398339148210073606|Check this out @B...|\n",
      "+--------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#vaccine', labelPredicted='#covid')\n",
      "========= 2021-05-28 20:14:30 =========\n",
      "+--------------+-------------------+--------------------+\n",
      "|         label|           tweet_id|          tweet_text|\n",
      "+--------------+-------------------+--------------------+\n",
      "|#stopasianhate|1398339228380143617|@BreitbartNews I ...|\n",
      "+--------------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#stopasianhate', labelPredicted='#china')\n",
      "========= 2021-05-28 20:14:40 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#biden|1398339552121700353|#███████ rejects ...|\n",
      "|#biden|1398339544810991616|Fauci and all oth...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#biden', labelPredicted='#covid')\n",
      "Row(labelOriginal='#biden', labelPredicted='#covid')\n",
      "========= 2021-05-28 20:14:50 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#biden|1398339417538928643|\"Biden budget inc...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#biden', labelPredicted='#biden')\n",
      "========= 2021-05-28 20:15:10 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#biden|1398339280603336705|#███████ #███████...|\n",
      "|#biden|1398339280116797447|@harveysp3cter @S...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#biden', labelPredicted='#stopasianhate')\n",
      "Row(labelOriginal='#biden', labelPredicted='#covid')\n",
      "========= 2021-05-28 20:15:20 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#biden|1398339241927528448|So if the #██████...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#biden', labelPredicted='#biden')\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "========= 2021-05-28 20:15:30 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#covid|1398340089294602240|As many graduatio...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "Row(labelOriginal='#covid', labelPredicted='#vaccine')\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
