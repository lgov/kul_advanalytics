{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6b0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "jsonSchema = StructType([\n",
    "    StructField('label', StringType(), True),\n",
    "    StructField('tweet_id', LongType(), True),\n",
    "    StructField('tweet_text', StringType(), True)\n",
    "])\n",
    "\n",
    "#replace the file path\n",
    "df=spark.read.format(\"json\").schema(jsonSchema).load(\"/Users/Pavel/Documents/KULeuven/Courses/AdvancedAnalyticsinBigDataWorld/spark/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "305ef95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import lit # for the dummy _transform\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "class RegexReplacerWritable(\n",
    "    Transformer, DefaultParamsReadable, DefaultParamsWritable,\n",
    "):\n",
    "    #value = Param(\n",
    "    #   Params._dummy(),\n",
    "    #   \"value\",\n",
    "    #   \"value to fill\",\n",
    "    #)\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(RegexReplacerWritable, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self):\n",
    "        \"\"\"\n",
    "        setParams(self, outputCols=None, value=0.0)\n",
    "        Sets params for this RegexReplacerWritable.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        #Converting all letters to lowercase\n",
    "        df = df.withColumn(\"tweet_text\",f.lower(f.col(\"tweet_text\")))\n",
    "        #removing punctuations, numbers, http and spaces\n",
    "        df = df.withColumn(\"tweet_text\",f.regexp_replace(f.col(\"tweet_text\"),'([^ a-zA-Z\\'])',''))\n",
    "        df = df.withColumn(\"tweet_text\",f.regexp_replace(f.col(\"tweet_text\"),'http.*?\\\\b',' '))\n",
    "        df = df.withColumn(\"tweet_text\",f.ltrim(f.regexp_replace(f.col(\"tweet_text\"),'[\\r\\n\\t\\f\\v ]+', ' ')))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15753023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import lit # for the dummy _transform\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class UDLemmatization(\n",
    "    Transformer, DefaultParamsReadable, DefaultParamsWritable,\n",
    "):\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(UDLemmatization, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self):\n",
    "        \"\"\"\n",
    "        setParams(self, outputCols=None, value=0.0)\n",
    "        Sets params for this RegexReplacerWritable.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        pandas_df = df.select(\"*\").toPandas()\n",
    "        pandas_df['lemmatized'] = pandas_df['words'].apply(\n",
    "                    lambda lst:[lemmatizer.lemmatize(word) for word in lst])\n",
    "        pandas_df['lemmatized']\n",
    "        df = spark.createDataFrame(pandas_df)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "\n",
    "m = __import__(\"__main__\"); \n",
    "setattr(m, 'UDLemmatization', UDLemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffd76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import lit # for the dummy _transform\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class UDShortWordsRemover(\n",
    "    Transformer, DefaultParamsReadable, DefaultParamsWritable,\n",
    "):\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(UDShortWordsRemover, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self):\n",
    "        \"\"\"\n",
    "        setParams(self, outputCols=None, value=0.0)\n",
    "        Sets params for this RegexReplacerWritable.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        df = df.withColumn(\"filtered2\", f.expr(\"filter(filtered, x -> not(length(x) < 3))\")).where(f.size(f.col(\"filtered2\")) > 0).drop(\"filtered\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7c15d",
   "metadata": {},
   "source": [
    "## Pipeline preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "323596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, IDF, HashingTF, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# 1. Regex Filter replacer\n",
    "regexrep = RegexReplacerWritable()\n",
    "\n",
    "# 2. Tokenizer - splitting words \n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"words\")\n",
    "\n",
    "# 3. Lemmatizer user defined\n",
    "lemmatizerUD = UDLemmatization()\n",
    "\n",
    "# 4. Stop Words Remover\n",
    "stopwordList = [\"u\",\"ur\", \"amp\", \"q\"] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\" ,stopWords=stopwordList)\n",
    "\n",
    "# 5. Short Words len <Â 3 user defined remover \n",
    "shortWordsremover = UDShortWordsRemover()\n",
    "\n",
    "# 6. Count Vectorizer\n",
    "cv = CountVectorizer(inputCol=\"filtered2\", outputCol=\"features\")\n",
    "\n",
    "# 7. IDF\n",
    "idf = IDF(inputCol = \"features\", outputCol = \"tf_idf_features\")\n",
    "\n",
    "# 8. String Indexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"labelIndex\")\n",
    "\n",
    "# 9. Logistic Regression\n",
    "lr = LogisticRegression(labelCol = \"labelIndex\", featuresCol = \"tf_idf_features\", maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# 10. Index to String, for now labels only, not prediction - TODO\n",
    "converter = IndexToString(inputCol=\"labelIndex\", outputCol=\"labelOriginal\")\n",
    "\n",
    "#create the pipeline\n",
    "pipeline = Pipeline(stages=[regexrep, tokenizer, lemmatizerUD, remover, shortWordsremover, cv, idf, label_stringIdx, lr, converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7bf5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85648ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit.write().overwrite().save('lr_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
