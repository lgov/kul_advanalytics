{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6b0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "jsonSchema = StructType([\n",
    "    StructField('label', StringType(), True),\n",
    "    StructField('tweet_id', LongType(), True),\n",
    "    StructField('tweet_text', StringType(), True)\n",
    "])\n",
    "\n",
    "#replace the file path\n",
    "df=spark.read.format(\"json\").schema(jsonSchema).load(\"/Users/Pavel/Documents/KULeuven/Courses/AdvancedAnalyticsinBigDataWorld/spark/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d016f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "\n",
    "#Converting all letters to lowercase\n",
    "df = df.withColumn(\"tweet_text\",f.lower(f.col(\"tweet_text\")))\n",
    "\n",
    "#removing punctuations, numbers, http and spaces\n",
    "df =df.withColumn(\"tweet_text\",f.regexp_replace(f.col(\"tweet_text\"),'([^ a-zA-Z\\'])',''))\n",
    "df = df.withColumn(\"tweet_text\",f.regexp_replace(f.col(\"tweet_text\"),'http.*?\\\\b',' '))\n",
    "df = df.withColumn(\"tweet_text\",f.ltrim(f.regexp_replace(f.col(\"tweet_text\"),'[\\r\\n\\t\\f\\v ]+', ' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7c15d",
   "metadata": {},
   "source": [
    "## Pipeline preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "#Splitting words\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"words\")\n",
    "\n",
    "#Removing stop words\n",
    "stopwordList = [\"u\",\"ur\", \"amp\", \"q\"] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\" ,stopWords=stopwordList)\n",
    "\n",
    "#Vectorizing\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"labelIndex\")\n",
    "\n",
    "#Logistic Regression\n",
    "lr = LogisticRegression(labelCol = \"labelIndex\", featuresCol = \"features\", maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "#create the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, label_stringIdx, lr])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7bf5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85648ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit.write().overwrite().save('lr_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5d40b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
