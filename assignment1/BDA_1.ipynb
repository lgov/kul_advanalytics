{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infrared-external",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import log_loss, roc_auc_score, plot_roc_curve, plot_confusion_matrix, confusion_matrix, make_scorer\n",
    "from sklearn.metrics import balanced_accuracy_score, dcg_score, roc_auc_score, average_precision_score\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "#from costcla.metrics import cost_loss\n",
    "\n",
    "from features import update_dataset_features, text_to_binary, add_extra_features, encode\n",
    "from eval_metrics import *\n",
    "\n",
    "pd.set_option(\"display.max_columns\",500)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"./train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, sep=\";\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"claim_date_occured\"] = pd.to_datetime(df[\"claim_date_occured\"], format=\"%Y%m%d\")\n",
    "#min(df[\"claim_date_occured\"].dt.year - df[\"policy_holder_year_birth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-frank",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"fraud\"], df[\"claim_vehicle_brand\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(i)\n",
    "    try:\n",
    "        df[i].plot(kind=\"hist\")\n",
    "        plt.show()\n",
    "        print(df[i].describe())\n",
    "    except TypeError:\n",
    "        values = df[i].value_counts()\n",
    "        if len(values) < 10:\n",
    "            values.plot(kind=\"bar\")\n",
    "            plt.show()\n",
    "            print(df[i].describe())\n",
    "        else:\n",
    "            print('*******too many values to plot*******************')\n",
    "            print(df[i].describe())\n",
    "    print('*************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-chorus",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, ohe = update_dataset_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26675336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-radiation",
   "metadata": {},
   "source": [
    "# Train test split + prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim_amount is currently dropped since poor performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:].drop(columns=[\"claim_amount\"]), df[\"fraud\"], test_size=.2, random_state=96)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(df.drop(columns=[\"fraud\", \"claim_amount\"]),\n",
    "                                                                    df[\"claim_amount\"], test_size=.2, random_state=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute remaining missing values with mode or mean on train set\n",
    "# here it could potentially make sense to include a third category (i.e. missing), although this would be a small cat\n",
    "\n",
    "# mode\n",
    "train_lang_mode = X_train[\"claim_language\"].mode()[0]\n",
    "train_vtype_mode = X_train[\"claim_vehicle_type\"].mode()[0]\n",
    "\n",
    "# mean\n",
    "train_premium_mean = X_train[\"policy_premium_100\"].mean()\n",
    "train_coverage_mean = X_train[\"policy_coverage_1000\"].mean()\n",
    "train_policy_holder_mean_age = X_train[\"policy_holder_age\"].mean()\n",
    "\n",
    "for train_set in [X_train, X_train_reg]:\n",
    "    train_set[\"claim_language\"].fillna(train_lang_mode, inplace=True)\n",
    "    train_set[\"claim_vehicle_type\"].fillna(train_vtype_mode, inplace=True)\n",
    "    train_set[\"policy_premium_100\"].fillna(train_premium_mean, inplace=True)\n",
    "    train_set[\"policy_coverage_1000\"].fillna(train_coverage_mean, inplace=True)\n",
    "    train_set[\"policy_holder_age\"].fillna(train_policy_holder_mean_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute remaining missing values with mode or mean from train set on test set\n",
    "# here it could potentially make sense to include a third category (i.e. missing), although this would be a small cat\n",
    "\n",
    "for test_set in [X_test, X_test_reg]:\n",
    "    # mode\n",
    "    test_set[\"claim_language\"].fillna(train_lang_mode, inplace=True)\n",
    "    test_set[\"claim_vehicle_type\"].fillna(train_vtype_mode, inplace=True)\n",
    "\n",
    "    # mean\n",
    "    test_set[\"policy_premium_100\"].fillna(train_premium_mean, inplace=True)\n",
    "    test_set[\"policy_coverage_1000\"].fillna(train_coverage_mean, inplace=True)\n",
    "    X_test[\"policy_holder_age\"].fillna(train_policy_holder_mean_age, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-procurement",
   "metadata": {},
   "source": [
    "# Predicting claim amount - DO NOT USE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-rochester",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression().fit(X_train_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set R squared: {}\".format(lm.score(X_train_reg, y_train_reg)))\n",
    "print(\"Testing set R squared: {}\".format(lm.score(X_test_reg, y_test_reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('intercept:', lm.intercept_)\n",
    "print('coef:', lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-humanity",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, max_depth=5, n_jobs=2, max_features=\"sqrt\", \n",
    "                           random_state=96, warm_start=True, bootstrap=True)\n",
    "model2 = rf.fit(X_train_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set R squared: {}\".format(model2.score(X_train_reg, y_train_reg)))\n",
    "print(\"Testing set R squared: {}\".format(model2.score(X_test_reg, y_test_reg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-bulgarian",
   "metadata": {},
   "source": [
    "# Base Logistic Regression model - off the shelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to predict claim_amount on the testset to evaluate true performance\n",
    "#X_test.drop(columns=[\"claim_amount\"], inplace=True)\n",
    "#X_test[\"claim_amount\"] = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "plot_roc_curve(clf, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf, X_test_scaled, y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-medication",
   "metadata": {},
   "source": [
    "# Logistic Regression model - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=0.2)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "X_train_scaled_resampled, y_train_resampled = pipeline.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before SMOTE and undersampling\n",
    "neg_length = len(y_train) - y_train.sum()\n",
    "pos_length = y_train.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")\n",
    "print(\"*********************************************************\")\n",
    "# after SMOTE and undersampling\n",
    "neg_length = len(y_train_resampled) - y_train_resampled.sum()\n",
    "pos_length = y_train_resampled.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_resampled = LogisticRegression(max_iter=500)\n",
    "clf_resampled.fit(X_train_scaled_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "plot_roc_curve(clf_resampled, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf_resampled, X_test_scaled, y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-child",
   "metadata": {},
   "source": [
    "# Random Forest - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=0.4)\n",
    "under = RandomUnderSampler(sampling_strategy=1)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before SMOTE and undersampling\n",
    "neg_length = len(y_train) - y_train.sum()\n",
    "pos_length = y_train.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")\n",
    "print(\"*********************************************************\")\n",
    "# after SMOTE and undersampling\n",
    "neg_length = len(y_train_resampled) - y_train_resampled.sum()\n",
    "pos_length = y_train_resampled.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=20, class_weight='balanced_subsample', random_state=9, n_jobs=-1)\n",
    "rf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf, X_test, y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-ending",
   "metadata": {},
   "source": [
    "# Balanced random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-fiber",
   "metadata": {},
   "source": [
    "See [here](https://imbalanced-learn.org/dev/references/generated/imblearn.ensemble.BalancedRandomForestClassifier.html#imblearn.ensemble.BalancedRandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=0.4)\n",
    "steps = [('o', over)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before SMOTE and undersampling\n",
    "neg_length = len(y_train) - y_train.sum()\n",
    "pos_length = y_train.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")\n",
    "print(\"*********************************************************\")\n",
    "# after SMOTE and undersampling\n",
    "neg_length = len(y_train_resampled) - y_train_resampled.sum()\n",
    "pos_length = y_train_resampled.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.60 savings\n",
    "bclf = BalancedRandomForestClassifier(n_estimators=10000, max_depth=5, random_state=9, \n",
    "                                      n_jobs=-1, class_weight=\"balanced_subsample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "bclf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(bclf, X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(bclf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(bclf, X_test, y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36dc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_train = df.iloc[X_train.index][\"claim_amount\"]\n",
    "cost_test = df.iloc[X_test.index][\"claim_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5987d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savings(X_train, y_train, X_test, y_test, cost_mat_train, cost_mat_test, model):\n",
    "    cost_train = costs(y_train, model.predict(X_train), cost_mat_train)\n",
    "    max_cost_train = max_costs(y_train, cost_mat_train)\n",
    "    cost_test = costs(y_test, model.predict(X_test), cost_mat_test)\n",
    "    max_cost_test = max_costs(y_test, cost_mat_test)\n",
    "    # max_cost is if we would label everything as negative (i.e. if we don't use our model)\n",
    "    # costs calculates how much costs our model makes (i.e. we still make costs but less since we catch some frauds)\n",
    "    # this ratio then shows the savings of our model vs. not using our model at all\n",
    "    print('train savings: ', np.round((max_cost_train - cost_train) / max_cost_train, 3))\n",
    "    print('test savings: ', np.round((max_cost_test - cost_test) / max_cost_test, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# savings train\n",
    "cost_train = eval_metrics.costs(y_train, bclf.predict(X_train), cost_mat_train)\n",
    "max_cost_train = eval_metrics.max_costs(y_train, cost_mat_train)\n",
    "cost_test = eval_metrics.costs(y_test, bclf.predict(X_test), cost_mat_test)\n",
    "max_cost_test = eval_metrics.max_costs(y_test, cost_mat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102094d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_cost is if we would label everything as negative (i.e. if we don't use our model)\n",
    "# costs calculates how much costs our model makes (i.e. we still make costs but less since we catch some frauds)\n",
    "# this ratio then shows the savings of our model vs. not using our model at all\n",
    "print('train savings: ', np.round((max_cost_train - cost_train) / max_cost_train, 3))\n",
    "print('test savings: ', np.round((max_cost_test - cost_test) / max_cost_test, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b45d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "savings(X_train, y_train, X_test, y_test, cost_mat_train, cost_mat_test, bclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-trauma",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70c0a7",
   "metadata": {},
   "source": [
    "see [here](https://xgboost.readthedocs.io/en/latest/parameter.html) for information on all parameters <br />\n",
    "see [here](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/cross_validation.py) and [here](https://github.com/tqchen/xgboost/tree/master/demo) for example of custom obj. function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_cost(y_true, y_pred):\n",
    "    cost_FN = df.iloc[y_true.index][\"claim_amount\"]\n",
    "    c_FP = 50\n",
    "    c_TP = 50\n",
    "    c_TN = 0\n",
    "    cost_mat = np.array([c_FP * np.ones(y_true.shape[0]), cost_FN, \n",
    "                             c_TP * np.ones(y_true.shape[0]), \n",
    "                             c_TN * np.ones(y_true.shape[0])]).T\n",
    "    cost_train = costs(y_true, y_pred, cost_mat)\n",
    "    max_cost = max_costs(y_true, cost_mat)\n",
    "    savings = (max_cost - cost_train) / max_cost\n",
    "    return savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_score(y, y_prob):\n",
    "    top_100_ranked_TP = pd.DataFrame(y_prob, y).reset_index().sort_values(by=0, ascending=False).iloc[:100][\"fraud\"].sum()\n",
    "    avg_fraud_rate = np.asarray(y).sum()/len(y)\n",
    "    lift = top_100_ranked_TP/avg_fraud_rate\n",
    "    return lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6461f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_length = len(y_train) - y_train.sum()\n",
    "pos_length = y_train.sum()\n",
    "ratio = neg_length/pos_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ebf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "# If you care only about the overall performance metric (AUC) of your prediction:\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators = 500, objective=\"binary:logistic\", base_score=.99, eval_metric=\"logloss\", seed=9, \n",
    "                            scale_pos_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you care about predicting the right probability:\n",
    "# ^---- this is what we should do (IMO) --> 0.69 savings\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators = 100, objective=\"binary:logistic\", eval_metric=\"auc\", seed=9,\n",
    "                            scale_pos_weight=ratio/5, reg_alpha=0.1, reg_lambda=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c061e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[:,-9:] = X_train.iloc[:,-9:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weighted claim_amount\n",
    "df[\"claim_amount_weighted\"] = [df.loc[i, \"claim_amount\"] * 1.3  if df.loc[i, 'fraud'] == 1 else df.loc[i, \"claim_amount\"] * 1 for i in df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cost based on weighted claim amount or unweighted\n",
    "cost = df.iloc[X_train.index][\"claim_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c056d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample-weighted XGB\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators = 100, objective=\"binary:logistic\", eval_metric='auc', min_child_weight=1, seed=9,\n",
    "                            sample_weight=cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf.fit(np.asarray(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfcc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set grid params\n",
    "params = {\n",
    "        'n_estimators': [100, 150, 200, 250, 300, 400, 500, 600],\n",
    "        'learning_rate': [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "        'min_child_weight': [1, 3, 5, 7, 10, 20, 30],\n",
    "        'reg_lambda': [0.5, 1, 1.5, 2, 3, 4, 5],\n",
    "        'subsample': [0.4, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'scale_pos_weight': [ratio/i for i in np.arange(1,11)] + [1],\n",
    "        'reg_alpha': [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "        'base_score': list(np.arange(0.1,1,0.09)),\n",
    "        'gamma': list(np.arange(0,22,3))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf2 = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric='aucpr', seed=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab90843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make grid search pipeline\n",
    "over = SMOTE(sampling_strategy=0.4, random_state=42)\n",
    "under = RandomUnderSampler(sampling_strategy=0.6, random_state=42)\n",
    "pipeline = make_pipeline(over, under, xgb_clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename params\n",
    "new_params = {'xgbclassifier__' + key: params[key] for key in params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make different custom scorers\n",
    "score = make_scorer(balanced_accuracy_score, greater_is_better=True)\n",
    "dcg_scorer = make_scorer(dcg_score, needs_proba=True)\n",
    "roc_scorer = make_scorer(roc_auc_score, greater_is_better=True,\n",
    "                             needs_threshold=True, average='weighted', sample_weight=cost)\n",
    "pr_score = make_scorer(average_precision_score, greater_is_better=True, \n",
    "                       needs_proba=True, average='weighted')\n",
    "lift_scorer = make_scorer(lift_score, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply randomized grid search CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=new_params, n_iter=600, \n",
    "                                   scoring=lift_scorer, n_jobs=-1, cv=cv, verbose=3, random_state=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e447ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit random search cv\n",
    "xgb_search = random_search.fit(np.asarray(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best params\n",
    "xgb_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dec26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results of cv\n",
    "pd.DataFrame(xgb_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save restuls in json\n",
    "import json\n",
    "with open('output_pr.json', 'w+') as f:\n",
    "    json.dump(xgb_search.best_params_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d57105",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric='aucpr', subsample= 0.8,\n",
    "                         scale_pos_weight= 29.17857142857143,\n",
    "                         reg_lambda= 1.5,\n",
    "                         reg_alpha= 0.2,\n",
    "                         n_estimators= 600,\n",
    "                         min_child_weight= 20,\n",
    "                         max_depth= 9,\n",
    "                         learning_rate= 0.01,\n",
    "                         gamma= 20,\n",
    "                         colsample_bytree= 0.8,\n",
    "                         base_score= 0.64,\n",
    "                           sample_weight=np.asarray(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf.fit(np.asarray(X_train_resampled), y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(xgb_clf, np.asarray(X_train), y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa688ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(xgb_clf, np.asarray(X_train), y_train)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e782a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[:,-9:] = X_test.iloc[:,-9:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(xgb_search, np.asarray(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39605261",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(xgb_search, np.asarray(X_test), y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "savings(np.asarray(X_train), y_train, np.asarray(X_test), y_test, cost_mat_train, cost_mat_test, xgb_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8681dd",
   "metadata": {},
   "source": [
    "# Set costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_FN_train = df.iloc[X_train.index][\"claim_amount\"]\n",
    "cost_FN_test = df.iloc[X_test.index][\"claim_amount\"]\n",
    "c_FP = 5\n",
    "c_TP = 5\n",
    "c_TN = 0\n",
    "cost_mat_train = np.array([c_FP * np.ones(X_train.shape[0]), cost_FN_train, \n",
    "                         c_TP * np.ones(X_train.shape[0]), \n",
    "                         c_TN * np.ones(X_train.shape[0])]).T\n",
    "\n",
    "cost_mat_test = np.array([c_FP * np.ones(X_test.shape[0]), cost_FN_test, \n",
    "                         c_TP * np.ones(X_test.shape[0]), \n",
    "                         c_TN * np.ones(X_test.shape[0])]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1051974c",
   "metadata": {},
   "source": [
    "# Cost-sensitive NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b437179",
   "metadata": {},
   "source": [
    "as seen in this [medium post](https://towardsdatascience.com/fraud-detection-with-cost-sensitive-machine-learning-24b8760d35d9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66713da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_input(y_train, c_FN):\n",
    "    y_str = pd.Series(y_train).reset_index(drop=True).apply(lambda x: str(int(x)))\n",
    "    c_FN_str = pd.Series(c_FN).reset_index(drop=True).apply(lambda x: '0' *\n",
    "                        (5-len(str(int(x)))) + str(int(x)))\n",
    "    return y_str + '.' + c_FN_str\n",
    "\n",
    "def custom_loss(c_FP, c_TP, c_TN):\n",
    "    def loss_function(y_input, y_pred):\n",
    "        y_true = K.round(y_input)\n",
    "        c_FN = (y_input - y_true) * 1e5\n",
    "        eps = 0.0001\n",
    "        y_pred = K.minimum(1.0 - eps, K.maximum(0.0 + eps, y_pred))\n",
    "        cost = y_true * (K.log(y_pred) * c_FN + K.log(1 - y_pred) * c_TP)\n",
    "        cost += (1 - y_true) * (K.log(1 - y_pred) * c_FP + K.log(y_pred) * c_TN)\n",
    "        return - K.mean(cost, axis=-1)\n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9404b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred, c_FP, c_TP, c_TN, c_FN):\n",
    "    return y_true * (c_FN*np.log(y_pred) + c_TP*np.log(1-y_pred)) + \\\n",
    "            (1 - y_true) * (c_FP*np.log(1-y_pred) + c_TN*np.log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = 0\n",
    "y_pred = 0.89\n",
    "c_FP = 200\n",
    "c_TP = 5\n",
    "c_TN = 0\n",
    "c_FN = 50000\n",
    "loss(y_true, y_pred, c_FP, c_TP, c_TN, c_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857fbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_input = create_y_input(y_train, cost).apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_FP = 80\n",
    "c_TP = 0\n",
    "c_TN = 0\n",
    "c_FN = cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fdc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array=np.asarray(X_train).astype(np.float32)\n",
    "y_train_array=np.asarray(y_train).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array=np.asarray(X_test).astype(np.float32)\n",
    "y_test_array=np.asarray(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "def ann(indput_dim, dropout=0.2):\n",
    "    model = Sequential([\n",
    "    Dense(units=50, input_dim=indput_dim, activation='relu'),\n",
    "    Dropout(dropout),\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dropout(dropout),\n",
    "    Dense(15, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "    return model\n",
    "clf = ann(indput_dim=X_train_array.shape[1], dropout=0.2)\n",
    "clf.compile(optimizer='adam', loss=custom_loss(c_FP, c_TP,\n",
    "            c_TN))\n",
    "clf.fit(X_train_array, y_input, batch_size=128, epochs=100, verbose=1)\n",
    "#clf.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c03472",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_ann = clf.predict(np.asarray(X_train).astype(np.float32), verbose=1)\n",
    "confusion_matrix(np.asarray(y_train), np.round(y_pred_train_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9331f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_ann = clf.predict(np.asarray(X_test).astype(np.float32), verbose=1)\n",
    "confusion_matrix(np.asarray(y_test), np.round(y_pred_test_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_mat_train = np.array([c_FP * np.ones(X_train.shape[0]), c_FN, \n",
    "                         c_TP * np.ones(X_train.shape[0]), \n",
    "                         c_TN * np.ones(X_train.shape[0])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec53805",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_FP = 80\n",
    "c_TP = 0\n",
    "c_TN = 0\n",
    "cost_mat_train = np.array([c_FP * np.ones(X_train.shape[0]), c_FN, \n",
    "                         c_TP * np.ones(X_train.shape[0]), \n",
    "                         c_TN * np.ones(X_train.shape[0])]).T\n",
    "\n",
    "cost_mat_test = np.array([c_FP * np.ones(X_test.shape[0]), cost_test, \n",
    "                         c_TP * np.ones(X_test.shape[0]), \n",
    "                         c_TN * np.ones(X_test.shape[0])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679529da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98768ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics.evaluate('ANN Cost Sensitive', y_train_array, y_test_array, y_pred_train_ann, \n",
    "                          y_pred_test_ann, cost_mat_train, cost_mat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80321937",
   "metadata": {},
   "source": [
    "# Cost-sensitive Logistic regression\n",
    "Doesn't work very well due to bugs in Costcla (not maintained very well) and incompatibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948eb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from costcla.metrics import *\n",
    "from costcla.models import CostSensitiveLogisticRegression, CostSensitiveRandomForestClassifier\n",
    "from costcla.sampling import smote, cost_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dfd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "X_train_resampled, y_train_resampled, cost_mat_resampled = smote(X_train.to_numpy(), y_train.to_numpy(), cost_mat=cost_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee579e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "X_train_resampled, y_train_resampled, cost_mat_resampled = cost_sampling(X_train.to_numpy(), y_train.to_numpy(), cost_mat=cost_matrix_train, \n",
    "                                                                         method='OverSampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before SMOTE and undersampling\n",
    "neg_length = len(y_train) - y_train.sum()\n",
    "pos_length = y_train.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")\n",
    "print(\"*********************************************************\")\n",
    "# after SMOTE and undersampling\n",
    "neg_length = len(y_train_resampled) - y_train_resampled.sum()\n",
    "pos_length = y_train_resampled.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57232ce2",
   "metadata": {},
   "source": [
    "We set administrative cost (for TP and FP) = 5 as in [Bahnsen et al.](https://ieeexplore.ieee.org/document/6784638)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(augmented, investigation_fee):\n",
    "    \"\"\"\n",
    "    augmented: train or test set augmented with dropped columns for claim_amount\n",
    "    investigation_fee: float, fee for investigating fraud as % of transaction\n",
    "    \"\"\"\n",
    "    TP_cost = [investigation_fee for i in range(len(augmented))]\n",
    "    FP_cost = [investigation_fee for i in range(len(augmented))]\n",
    "    FN_cost = augmented[\"claim_amount\"]\n",
    "    TN_cost = [0 for i in range(len(augmented))]\n",
    "    cost_matrix = np.array([FP_cost, FN_cost, TP_cost, TN_cost]).T\n",
    "    return cost_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design cost matrix\n",
    "investigation_fee = 5.0\n",
    "train_augmented = df.iloc[X_train.index].copy()\n",
    "test_augmented = df.iloc[X_test.index].copy()\n",
    "\n",
    "cost_matrix_train = calculate_cost(train_augmented, investigation_fee)\n",
    "cost_matrix_test = calculate_cost(test_augmented, investigation_fee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc03df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train cost sensitive classifier\n",
    "cs_clf = CostSensitiveLogisticRegression()\n",
    "cs_clf.fit(X_train_scaled, y_train, cost_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brf\n",
    "savings_score(y_test, bclf.predict(X_test), cost_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cost_loss(y_test, clf.predict(X_test_scaled), cost_matrix_test))\n",
    "print(savings_score(y_test, clf.predict(X_test_scaled), cost_matrix_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cs_clf.predict_proba(X_test_scaled)[:,1])\n",
    "print(cs_clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639506ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cost_loss(y_test, cs_clf.predict(X_test_scaled), cost_matrix_test))\n",
    "print(savings_score(y_test, cs_clf.predict(X_test_scaled), cost_matrix_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d52c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classification_metrics(y_test, cs_clf.predict(X_test_scaled), cs_clf.predict_proba(X_test_scaled)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d51e6c",
   "metadata": {},
   "source": [
    "# Cost-sensitive RF\n",
    "Doesn't work very well due to bugs in Costcla and incompatibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a740e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76995510",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551160c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train cost sensitive classifier\n",
    "cs_rf = CostSensitiveRandomForestClassifier(n_estimators=10000, max_features=\"sqrt\", n_jobs=-1, combination=\"stacking_proba\")\n",
    "cs_rf.fit(X_train_scaled, y_train, cost_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119debb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cost_loss(y_test, cs_rf.predict(X_test_scaled), cost_matrix_test))\n",
    "print(savings_score(y_test, cs_rf.predict(X_test_scaled), cost_matrix_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(savings_score(y_train, cs_rf.predict(X_train_scaled), cost_matrix_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdeecdb",
   "metadata": {},
   "source": [
    "# Cost-sensitive sampling\n",
    "Not actually used, just to try out some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65909a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fraud_cost = cost.iloc[np.where(y_train == 1)].mean()\n",
    "investigation_cost = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_FP = 5\n",
    "c_TP = 5\n",
    "c_TN = 0\n",
    "c_FN = avg_fraud_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(c_FP - c_TN) / ((c_FP - c_TN) + (c_FN - c_TP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fraud_cost/investigation_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1215767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_rule(predicted_prob):\n",
    "    FN_cost = predicted_prob[:,0]*0 + predicted_prob[:,1]*avg_fraud_cost\n",
    "    FP_cost = predicted_prob[:,0]*investigation_cost + predicted_prob[:,1]*investigation_cost\n",
    "    return (FN_cost > FP_cost).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76924ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, clf_rule(clf.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-fiber",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path = r\"./test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_set = pd.read_csv(submit_path, sep=\";\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert binary text variables into binary: {\"Y\":1, \"N\":0}\n",
    "for i in [\"claim_liable\", \"claim_police\", \"driver_injured\"]:\n",
    "    text_to_binary(i, \"Y\", \"N\", submit_set)\n",
    "# {\"P\":1, \"N\":0}\n",
    "text_to_binary(\"claim_alcohol\", \"P\", \"N\", submit_set)\n",
    "# {\"car\":1, \"van\":0}\n",
    "text_to_binary(\"claim_vehicle_type\", \"car\", \"van\", submit_set)\n",
    "# {\"M\":1, \"F\":0}\n",
    "text_to_binary(\"policy_holder_form\", \"M\", \"F\", submit_set)\n",
    "# {\"B\":1, \"N\":0}\n",
    "text_to_binary(\"policy_holder_country\", \"B\", \"N\", submit_set)\n",
    "# make claim_lang binary (currently 1:Dutch, 2:Fr) -> 0: Dutch and 1: French\n",
    "submit_set[\"claim_language\"] = submit_set[\"claim_language\"] - 1 \n",
    "\n",
    "# add buckets for vehicle power\n",
    "submit_set[\"vpower_buckets\"] = pd.qcut(submit_set[\"claim_vehicle_power\"], 5)\n",
    "\n",
    "# add provinces based on postal code\n",
    "postal_bins = [999, 1299, 1499, 1999, 2999, 3499, 3999, 4999, 5999, 6599, 6999, 7999, 8999, 9999]\n",
    "postal_label = [\"brussel\", \"waals_brabant\", \"vlaams_brabant\", 'Antwerpen', 'vlaams_brabant', 'limburg', \n",
    "                'luik', 'namen', 'henegouwen', 'luxemburg', 'henegouwen', 'w-vlaanderen', 'o-vlaanderen']\n",
    "submit_set[\"province\"] = pd.cut(submit_set[\"claim_postal_code\"], postal_bins, labels=postal_label, ordered=False)\n",
    "\n",
    "# add feature that describes if policy holders postal code is same as claim postal code\n",
    "submit_set[\"diff_postal_code\"] = (submit_set[\"policy_holder_postal_code\"] == submit_set[\"claim_postal_code\"]).astype(float)\n",
    "\n",
    "# get dummies for cat vars\n",
    "submit_set = encode(ohe, submit_set, ['claim_cause', 'vpower_buckets', 'province'])\n",
    "#df = encode_ph_postal_code(phpc_ohe, df)\n",
    "\n",
    "# format date\n",
    "YYYYMMDD_date_columns = [\"claim_date_registered\",\n",
    "                         \"claim_date_occured\"]\n",
    "for i in YYYYMMDD_date_columns:\n",
    "    submit_set[i] = pd.to_datetime(submit_set[i], format=\"%Y%m%d\")\n",
    "\n",
    "# remove extreme value\n",
    "submit_set[\"claim_vehicle_date_inuse\"].replace(to_replace=270505.0, value= np.nan, inplace=True)\n",
    "\n",
    "YYYYMM_columns = [\"claim_vehicle_date_inuse\", \n",
    "                  \"policy_date_start\",\n",
    "                  \"policy_date_next_expiry\",\n",
    "                  \"policy_date_last_renewed\"]\n",
    "for i in YYYYMM_columns:\n",
    "    submit_set[i] = pd.to_datetime(submit_set[i], format=\"%Y%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the extra features just like we did for the training set\n",
    "submit_set = add_extra_features(submit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide the claim_id column as index so that it's not used as covariate for the prediction, but we can recover\n",
    "# it later as we need claim_id in the output .csv file\n",
    "submit_set = submit_set.set_index('claim_id')\n",
    "#df = df[X_train.drop(columns=[\"claim_amount\"]).columns]\n",
    "submit_set = submit_set[X_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eef04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_set.to_csv('submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76013259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "data = {\n",
    "    \"data\":\n",
    "    [\n",
    "        {\n",
    "            'Column2': \"example_value\",\n",
    "            'claim_liable': \"0\",\n",
    "            'claim_num_injured': \"0\",\n",
    "            'claim_num_third_parties': \"0\",\n",
    "            'claim_num_vehicles': \"0\",\n",
    "            'claim_police': \"0\",\n",
    "            'claim_language': \"0\",\n",
    "            'claim_vehicle_type': \"0\",\n",
    "            'policy_holder_form': \"0\",\n",
    "            'policy_holder_country': \"0\",\n",
    "            'policy_num_changes': \"0\",\n",
    "            'policy_num_claims': \"0\",\n",
    "            'policy_premium_100': \"0\",\n",
    "            'policy_coverage_1000': \"0\",\n",
    "            'diff_postal_code': \"0\",\n",
    "            'claim_cause_animal': \"0\",\n",
    "            'claim_cause_fire': \"0\",\n",
    "            'claim_cause_other': \"0\",\n",
    "            'claim_cause_theft': \"0\",\n",
    "            'claim_cause_traffic accident': \"0\",\n",
    "            'claim_cause_vandalism': \"0\",\n",
    "            'claim_cause_weather': \"0\",\n",
    "            'claim_cause_windows': \"0\",\n",
    "            'vpower_buckets_(-0.001, 55.0]': \"0\",\n",
    "            'vpower_buckets_(55.0, 66.0]': \"0\",\n",
    "            'vpower_buckets_(66.0, 81.0]': \"0\",\n",
    "            'vpower_buckets_(81.0, 100.0]': \"0\",\n",
    "            'vpower_buckets_(100.0, 426.0]': \"0\",\n",
    "            'vpower_buckets_nan': \"0\",\n",
    "            'province_Antwerpen': \"0\",\n",
    "            'province_brussel': \"0\",\n",
    "            'province_henegouwen': \"0\",\n",
    "            'province_limburg': \"0\",\n",
    "            'province_luik': \"0\",\n",
    "            'province_luxemburg': \"0\",\n",
    "            'province_namen': \"0\",\n",
    "            'province_o-vlaanderen': \"0\",\n",
    "            'province_vlaams_brabant': \"0\",\n",
    "            'province_w-vlaanderen': \"0\",\n",
    "            'province_waals_brabant': \"1\",\n",
    "            'claim_vehicle_id_count': \"0\",\n",
    "            'policy_holder_id_count': \"0\",\n",
    "            'driver_id_count': \"0\",\n",
    "            'driver_vehicle_id_count': \"0\",\n",
    "            'third_party_1_id_count': \"0\",\n",
    "            'third_party_1_vehicle_id_count': \"0\",\n",
    "            'blacklisted_expert_id': \"false\",\n",
    "            'policy_holder_age': \"0\",\n",
    "            'pct1': \"0\",\n",
    "            'pct2': \"0\",\n",
    "            'pct3': \"0\",\n",
    "            'pct4': \"0\",\n",
    "            'pct5': \"0\",\n",
    "            'pct6': \"0\",\n",
    "            'pct7': \"0\",\n",
    "            'pct8': \"0\",\n",
    "            'pct9': \"0\",\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'http://be62a776-8b23-41b0-a6d2-0f61a6e64323.francecentral.azurecontainer.io/score'\n",
    "api_key = '' # Replace this with the API key for the web service\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(json.loads(error.read().decode(\"utf8\", 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute remaining missing values with mode or mean on train set\n",
    "# here it could potentially make sense to include a third category (i.e. missing), although this would be a small cat\n",
    "\n",
    "# mode\n",
    "submit_set[\"claim_language\"].fillna(train_lang_mode, inplace=True)\n",
    "submit_set[\"claim_vehicle_type\"].fillna(train_vtype_mode, inplace=True)\n",
    "\n",
    "# mean\n",
    "submit_set[\"policy_premium_100\"].fillna(train_premium_mean, inplace=True)\n",
    "submit_set[\"policy_coverage_1000\"].fillna(train_coverage_mean, inplace=True)\n",
    "submit_set[\"policy_holder_age\"].fillna(train_policy_holder_mean_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert submit_set.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While we don't have a model yet to predict claim_amount, set it to 0.0\n",
    "# TODO: Replace this with the predicted response variable of the regression model on the submission set.\n",
    "#df['claim_amount'] = model2.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_scaled = scaler.transform(submit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_not_scaled = submit_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final submission set initialization\n",
    "submission = submit_set.reset_index()[['claim_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logistic regression\n",
    "submission[\"prediction\"] = clf.predict_proba(submit_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logistic regression with SMOTE\n",
    "submission[\"prediction\"] = clf_resampled.predict_proba(submit_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rf with SMOTE\n",
    "submission[\"prediction\"] = rf.predict_proba(submit_not_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for balanced random forest\n",
    "submission[\"prediction\"] = bclf.predict_proba(submit_not_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xgb\n",
    "submit_not_scaled.iloc[:,-9:] = submit_not_scaled.iloc[:,-9:].astype(float)\n",
    "submission[\"prediction\"] = xgb_clf.predict_proba(submit_not_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af772615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cost-sensitive xgb\n",
    "submit_not_scaled.iloc[:,-9:] = submit_not_scaled.iloc[:,-9:].astype(float)\n",
    "submission[\"prediction\"] = xgb_clf.predict_proba(np.asarray(submit_not_scaled))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cost-sensitive ann\n",
    "submission[\"prediction\"] = clf.predict(np.asarray(submit_not_scaled).astype(np.float32), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a56194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for randomsearch xgb\n",
    "submit_not_scaled.iloc[:,-9:] = submit_not_scaled.iloc[:,-9:].astype(float)\n",
    "submission[\"PROB\"] = xgb_search.predict_proba(np.asarray(submit_not_scaled))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns = [\"ID\", \"PROB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cfa8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_V0.53.csv\", sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
