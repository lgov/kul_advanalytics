{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "connected-argument",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "* feature engineering\n",
    "* cost threshold chaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-newcastle",
   "metadata": {},
   "source": [
    "# Information tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-external",
   "metadata": {},
   "source": [
    "* For more info on SMOTE see [here](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import log_loss, roc_auc_score, plot_roc_curve, plot_confusion_matrix\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#import lightgbm as xgb\n",
    "\n",
    "from features import update_dataset_features, text_to_binary, add_extra_features, encode_claim_cause\n",
    "\n",
    "pd.set_option(\"display.max_columns\",500)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"./train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, sep=\";\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"claim_date_occured\"] = pd.to_datetime(df[\"claim_date_occured\"], format=\"%Y%m%d\")\n",
    "#min(df[\"claim_date_occured\"].dt.year - df[\"policy_holder_year_birth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-frank",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"fraud\"], df[\"claim_vehicle_brand\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(i)\n",
    "    try:\n",
    "        df[i].plot(kind=\"hist\")\n",
    "        plt.show()\n",
    "        print(df[i].describe())\n",
    "    except TypeError:\n",
    "        values = df[i].value_counts()\n",
    "        if len(values) < 10:\n",
    "            values.plot(kind=\"bar\")\n",
    "            plt.show()\n",
    "            print(df[i].describe())\n",
    "        else:\n",
    "            print('*******too many values to plot*******************')\n",
    "            print(df[i].describe())\n",
    "    print('*************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-chorus",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data in cell below to speed up and skip this step\n",
    "df, claim_cause_ohe = update_dataset_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-radiation",
   "metadata": {},
   "source": [
    "# Train test split + prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim_amount is currently dropped since poor performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:].drop(columns=[\"claim_amount\"]), df[\"fraud\"], test_size=.2, random_state=96)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(df.drop(columns=[\"fraud\", \"claim_amount\"]),\n",
    "                                                                    df[\"claim_amount\"], test_size=.2, random_state=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute remaining missing values with mode or mean on train set\n",
    "# here it could potentially make sense to include a third category (i.e. missing), although this would be a small cat\n",
    "\n",
    "# mode\n",
    "train_lang_mode = X_train[\"claim_language\"].mode()[0]\n",
    "train_vtype_mode = X_train[\"claim_vehicle_type\"].mode()[0]\n",
    "\n",
    "# mean\n",
    "train_premium_mean = X_train[\"policy_premium_100\"].mean()\n",
    "train_coverage_mean = X_train[\"policy_coverage_1000\"].mean()\n",
    "#train_policy_holder_mean_age = X_train[\"policy_holder_age\"].mean()\n",
    "\n",
    "for train_set in [X_train, X_train_reg]:\n",
    "    train_set[\"claim_language\"].fillna(train_lang_mode, inplace=True)\n",
    "    train_set[\"claim_vehicle_type\"].fillna(train_vtype_mode, inplace=True)\n",
    "    train_set[\"policy_premium_100\"].fillna(train_premium_mean, inplace=True)\n",
    "    train_set[\"policy_coverage_1000\"].fillna(train_coverage_mean, inplace=True)\n",
    "    # train_set[\"policy_holder_age\"].fillna(train_policy_holder_mean_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute remaining missing values with mode or mean from train set on test set\n",
    "# here it could potentially make sense to include a third category (i.e. missing), although this would be a small cat\n",
    "\n",
    "for test_set in [X_test, X_test_reg]:\n",
    "    # mode\n",
    "    test_set[\"claim_language\"].fillna(train_lang_mode, inplace=True)\n",
    "    test_set[\"claim_vehicle_type\"].fillna(train_vtype_mode, inplace=True)\n",
    "\n",
    "    # mean\n",
    "    test_set[\"policy_premium_100\"].fillna(train_premium_mean, inplace=True)\n",
    "    test_set[\"policy_coverage_1000\"].fillna(train_coverage_mean, inplace=True)\n",
    "    # X_test[\"policy_holder_age\"].fillna(train_policy_holder_mean_age, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-procurement",
   "metadata": {},
   "source": [
    "# Predicting claim amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-rochester",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression().fit(X_train_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set R squared: {}\".format(lm.score(X_train_reg, y_train_reg)))\n",
    "print(\"Testing set R squared: {}\".format(lm.score(X_test_reg, y_test_reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('intercept:', lm.intercept_)\n",
    "print('coef:', lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-humanity",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, max_depth=5, n_jobs=2, max_features=\"sqrt\", \n",
    "                           random_state=96, warm_start=True, bootstrap=True)\n",
    "model2 = rf.fit(X_train_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set R squared: {}\".format(model2.score(X_train_reg, y_train_reg)))\n",
    "print(\"Testing set R squared: {}\".format(model2.score(X_test_reg, y_test_reg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-bulgarian",
   "metadata": {},
   "source": [
    "# Base Logistic Regression model - off the shelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to predict claim_amount on the testset to evaluate true performance\n",
    "#X_test.drop(columns=[\"claim_amount\"], inplace=True)\n",
    "#X_test[\"claim_amount\"] = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "plot_roc_curve(clf, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf, X_test_scaled, y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-medication",
   "metadata": {},
   "source": [
    "# Logistic Regression model - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=0.05)\n",
    "under = RandomUnderSampler(sampling_strategy=0.3)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "X_train_scaled_resampled, y_train_resampled = pipeline.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before SMOTE and undersampling\n",
    "neg_length = len(y_train) - y_train.sum()\n",
    "pos_length = y_train.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")\n",
    "print(\"*********************************************************\")\n",
    "# after SMOTE and undersampling\n",
    "neg_length = len(y_train_resampled) - y_train_resampled.sum()\n",
    "pos_length = y_train_resampled.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_resampled = LogisticRegression(max_iter=500)\n",
    "clf_resampled.fit(X_train_scaled_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "plot_roc_curve(clf_resampled, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf_resampled, X_test_scaled, y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-child",
   "metadata": {},
   "source": [
    "# Random Forest - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=0.2)\n",
    "under = RandomUnderSampler(sampling_strategy=0.3)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before SMOTE and undersampling\n",
    "neg_length = len(y_train) - y_train.sum()\n",
    "pos_length = y_train.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")\n",
    "print(\"*********************************************************\")\n",
    "# after SMOTE and undersampling\n",
    "neg_length = len(y_train_resampled) - y_train_resampled.sum()\n",
    "pos_length = y_train_resampled.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10000, max_depth=5, warm_start=True, random_state=9, n_jobs=2)\n",
    "rf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf, X_test, y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-ending",
   "metadata": {},
   "source": [
    "# Balanced random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-fiber",
   "metadata": {},
   "source": [
    "See [here](https://imbalanced-learn.org/dev/references/generated/imblearn.ensemble.BalancedRandomForestClassifier.html#imblearn.ensemble.BalancedRandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=0.4)\n",
    "steps = [('o', over)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before SMOTE and undersampling\n",
    "neg_length = len(y_train) - y_train.sum()\n",
    "pos_length = y_train.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")\n",
    "print(\"*********************************************************\")\n",
    "# after SMOTE and undersampling\n",
    "neg_length = len(y_train_resampled) - y_train_resampled.sum()\n",
    "pos_length = y_train_resampled.sum()\n",
    "print(f\"Majority class (0): {neg_length}\")\n",
    "print(f\"Minority class (1): {pos_length}\")\n",
    "print(f\"ratio: {np.round(pos_length/neg_length,5)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "bclf = BalancedRandomForestClassifier(n_estimators=10000, max_depth=5, random_state=9, \n",
    "                                      n_jobs=2, class_weight=\"balanced_subsample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "bclf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(bclf, X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(bclf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(bclf, X_test, y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-trauma",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70c0a7",
   "metadata": {},
   "source": [
    "see [here](https://xgboost.readthedocs.io/en/latest/parameter.html) for information on all parameters <br />\n",
    "see [here](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/cross_validation.py) for example of custom obj. function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6461f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_length = len(y_train) - y_train.sum()\n",
    "pos_length = y_train.sum()\n",
    "ratio = neg_length/pos_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ebf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "# If you care only about the overall performance metric (AUC) of your prediction:\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators = 500, objective=\"binary:logistic\", base_score=.99, eval_metric=\"logloss\", seed=9, \n",
    "                            scale_pos_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you care about predicting the right probability:\n",
    "# ^---- this is what we should do (IMO)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators = 100, objective=\"binary:logistic\", eval_metric=\"auc\", seed=9, \n",
    "                            scale_pos_weight=ratio/5, reg_alpha=0.3, reg_lambda=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c061e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[:,-9:] = X_train.iloc[:,-9:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb85d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(xgb_clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa688ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(xgb_clf, X_train, y_train)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e782a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[:,-9:] = X_test.iloc[:,-9:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(xgb_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39605261",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(xgb_clf, X_test, y_test)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-fiber",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path = r\"./test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_set = pd.read_csv(submit_path, sep=\";\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = submit_set\n",
    "# convert binary text variables into binary: {\"Y\":1, \"N\":0}\n",
    "for i in [\"claim_liable\", \"claim_police\", \"driver_injured\"]:\n",
    "    text_to_binary(i, \"Y\", \"N\", df)\n",
    "# {\"P\":1, \"N\":0}\n",
    "text_to_binary(\"claim_alcohol\", \"P\", \"N\", df)\n",
    "# {\"car\":1, \"van\":0}\n",
    "text_to_binary(\"claim_vehicle_type\", \"car\", \"van\", df)\n",
    "# {\"M\":1, \"F\":0}\n",
    "text_to_binary(\"policy_holder_form\", \"M\", \"F\", df)\n",
    "# {\"B\":1, \"N\":0}\n",
    "text_to_binary(\"policy_holder_country\", \"B\", \"N\", df)\n",
    "# make claim_lang binary (currently 1:Dutch, 2:Fr) -> 0: Dutch and 1: French\n",
    "df[\"claim_language\"] = df[\"claim_language\"] - 1 \n",
    "\n",
    "# get dummies for cat vars\n",
    "df = encode_claim_cause(claim_cause_ohe, df)\n",
    "#df = encode_ph_postal_code(phpc_ohe, df)\n",
    "\n",
    "# format date\n",
    "YYYYMMDD_date_columns = [\"claim_date_registered\",\n",
    "                         \"claim_date_occured\"]\n",
    "for i in YYYYMMDD_date_columns:\n",
    "    df[i] = pd.to_datetime(df[i], format=\"%Y%m%d\")\n",
    "\n",
    "# remove extreme value\n",
    "df[\"claim_vehicle_date_inuse\"].replace(to_replace=270505.0, value= np.nan, inplace=True)\n",
    "\n",
    "YYYYMM_columns = [\"claim_vehicle_date_inuse\", \n",
    "                  \"policy_date_start\",\n",
    "                  \"policy_date_next_expiry\",\n",
    "                  \"policy_date_last_renewed\"]\n",
    "for i in YYYYMM_columns:\n",
    "    df[i] = pd.to_datetime(df[i], format=\"%Y%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the extra features just like we did for the training set\n",
    "df = add_extra_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide the claim_id column as index so that it's not used as covariate for the prediction, but we can recover\n",
    "# it later as we need claim_id in the output .csv file\n",
    "df = df.set_index('claim_id')\n",
    "#df = df[X_train.drop(columns=[\"claim_amount\"]).columns]\n",
    "df = df[X_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute remaining missing values with mode or mean on train set\n",
    "# here it could potentially make sense to include a third category (i.e. missing), although this would be a small cat\n",
    "\n",
    "# mode\n",
    "df[\"claim_language\"].fillna(train_lang_mode, inplace=True)\n",
    "df[\"claim_vehicle_type\"].fillna(train_vtype_mode, inplace=True)\n",
    "\n",
    "# mean\n",
    "df[\"policy_premium_100\"].fillna(train_premium_mean, inplace=True)\n",
    "df[\"policy_coverage_1000\"].fillna(train_coverage_mean, inplace=True)\n",
    "# df[\"policy_holder_age\"].fillna(train_policy_holder_mean_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While we don't have a model yet to predict claim_amount, set it to 0.0\n",
    "# TODO: Replace this with the predicted response variable of the regression model on the submission set.\n",
    "#df['claim_amount'] = model2.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_scaled = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_not_scaled = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final submission set initialization\n",
    "submission = df.reset_index()[['claim_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logistic regression\n",
    "submission[\"prediction\"] = clf.predict_proba(submit_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logistic regression with SMOTE\n",
    "submission[\"prediction\"] = clf_resampled.predict_proba(submit_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rf with SMOTE\n",
    "submission[\"prediction\"] = rf.predict_proba(submit_not_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for balanced random forest\n",
    "submission[\"prediction\"] = bclf.predict_proba(submit_not_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xgb\n",
    "submit_not_scaled.iloc[:,-9:] = submit_not_scaled.iloc[:,-9:].astype(float)\n",
    "submission[\"prediction\"] = xgb_clf.predict_proba(submit_not_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns = [\"ID\", \"PROB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_V0.28.csv\", sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
